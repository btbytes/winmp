
Embeddings are mathematical representations of phrases that capture the semantic meaning as vector of numerical values.

Phrases with similar semantic meanings will have similar vector representations => the *distance* between these vectors is going to be small. 

One common use of embeddings is [[Semantic Similarity Search]] -- If I have *knowledge library* consisting of phrases and I receive a question from a user, I can locate the most relevant information in my library by finding the data that is closer to the user's query.

In contrast,  [[Full Text Search]] only returns exact matches. 
